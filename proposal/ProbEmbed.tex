\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{amsfonts}
\usepackage{graphicx}%
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{titling}
\usepackage{stmaryrd}
\usepackage{enumitem}
\usepackage[margin=2.5cm]{geometry}

\theoremstyle{plain} \numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{finalremark}[theorem]{Final Remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}{Question} 

\setlength{\parskip}{0.5 \baselineskip}%
\setlength{\headheight}{15pt}
\setlength{\droptitle}{-100pt}

\titlespacing{\paragraph}{%
  0pt}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

\pagestyle{fancy}\lhead{September 2014} \rhead{ProbEmbed}
\chead{{\large{Proposal}}} \lfoot{} \rfoot{\bf \thepage} \cfoot{}

\newcounter{list}

\title{Learning Embeddings with Dirichlet Processes and Heavy Tails}
\author{%
  Moontae Lee \\ \href{ml2255@cornell.edu}{ml2255@cornell.edu}
  \and Adith Swaminathan\\ \href{fa234@cornell.edu}{fa234@cornell.edu}
  \and Chenhao Tan\\ \href{chenhao@cs.cornell.edu}{ct425@cornell.edu} 
}




\begin{document}
\maketitle


\begin{question}{What are you trying to do?}\end{question}
\vspace{-10px}
$\Rightarrow$ We are developing a generative model of word embeddings that
\begin{itemize}
  \vspace{-10px}
  \setlength\itemsep{1px}
  \setlength{\itemindent}{0.4in}
  \item Takes as input $D$: all observed text
  \item Outputs an embedding $X$: words situated in a vector space
\end{itemize}
\vspace{3px}


\begin{question}{Why is it hard?}\end{question}
\vspace{-10px}
$\Rightarrow$ We want our embeddings to satisfy the following properties simultaneously:
\begin{itemize}
  \vspace{-10px}
  \setlength\itemsep{1px}
  \setlength{\itemindent}{0.4in}
  \item Words \emph{cluster around concepts}
  \item Distances/directions in the embedding space directly model data distribution
  \item Scalable to learn
  \item Recover good performance of existing embeddings in downstream text analysis tasks.
\end{itemize}
\vspace{3px}


\begin{question}{How is it being done today?}\end{question}
\vspace{-10px}
\begin{itemize}
  \vspace{-10px}
  \setlength\itemsep{1px}
  \setlength{\itemindent}{0.4in}
  \item Try to optimize language model, context model, or ranking margin objectives.
  \item Have various evaluation tasks which are not aligned with the learning processes.
  \item Visualize learned embeddings and manually verify clusters with similar semantics/syntax.
\end{itemize}
\vspace{3px}


\begin{question}{What has changed that makes it possible to do something else?}\end{question}
\vspace{-10px}
\begin{itemize}
  \vspace{-10px}
  \setlength\itemsep{1px}
  \setlength{\itemindent}{0.4in}
  \item Small variance limits of Dirichlet Processes that can be optimized tractably, giving a clustering prior.
  \item Techniques to optimize a \textit{Heavy-tailed distribution} to properly model small probabilities. 
%  \item Put every ingredient on top of probabilistic story,  clearly defining \textit{good embeddings}.
\end{itemize}
\vspace{3px}


\begin{question}{How will you know whether you are doing a good job?}\end{question}
\vspace{-10px}
\begin{itemize}
  \vspace{-10px}
  \setlength\itemsep{1px}
  \setlength{\itemindent}{0.4in}
  \item Intrinsic evaluation based on the generative model, e.g., held-out data perplexity (not adequate).
  \item Find appropriate external evaluation tasks (e.g., NLP applications).
  \item User studies using the discovered embeddings.
\end{itemize}
\vspace{3px}


\begin{question}{Who will want to know what your results are?}\end{question}
\vspace{-10px}
$\Rightarrow$ People who are interested in representation learning, topic modeling,
and broadly text processing/information retrieval for exploratory data analysis.

\paragraph{Topic modeling with word embeddings.} A plausible generative model is to introduce the idea of
good concept clustering into topic modeling.
This approach replaces current multinomial distribution from topic to word with a heavy-tailed distribution from cluster centers.

% \bibliographystyle{alpha}
% \bibliography{ProbEmbed}

\end{document}
